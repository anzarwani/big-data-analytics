{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4017301,"sourceType":"datasetVersion","datasetId":2380926},{"sourceId":8412025,"sourceType":"datasetVersion","datasetId":5006719}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/anzarwani2/pyspark-beginner-tutorial?scriptVersionId=196733461\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-15T10:16:22.46626Z","iopub.execute_input":"2024-09-15T10:16:22.466753Z","iopub.status.idle":"2024-09-15T10:16:22.930388Z","shell.execute_reply.started":"2024-09-15T10:16:22.466706Z","shell.execute_reply":"2024-09-15T10:16:22.929278Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/air-quality/Air_Quality.csv\n/kaggle/input/electric-power-consumption/powerconsumption.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 1. Setting Up PySpark","metadata":{}},{"cell_type":"code","source":"!pip install pyspark","metadata":{"execution":{"iopub.status.busy":"2024-09-15T10:16:22.932259Z","iopub.execute_input":"2024-09-15T10:16:22.9328Z","iopub.status.idle":"2024-09-15T10:17:18.613781Z","shell.execute_reply.started":"2024-09-15T10:16:22.932757Z","shell.execute_reply":"2024-09-15T10:17:18.6123Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting pyspark\n  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\nBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812364 sha256=548d4586201ed49a0c92f60f0671a417a16a45d3bbae7985999a81de0887cb26\n  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\nSuccessfully built pyspark\nInstalling collected packages: pyspark\nSuccessfully installed pyspark-3.5.2\n","output_type":"stream"}]},{"cell_type":"code","source":"from pyspark.sql import SparkSession","metadata":{"execution":{"iopub.status.busy":"2024-09-15T10:17:18.616299Z","iopub.execute_input":"2024-09-15T10:17:18.616806Z","iopub.status.idle":"2024-09-15T10:17:18.735318Z","shell.execute_reply.started":"2024-09-15T10:17:18.616749Z","shell.execute_reply":"2024-09-15T10:17:18.734274Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"spark = SparkSession.builder.master(\"local[1]\") \\\n                    .appName('pyspark tutorial') \\\n                    .getOrCreate()\nprint(spark.sparkContext)\nprint(\"Spark App Name : \"+ spark.sparkContext.appName)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T10:17:18.738379Z","iopub.execute_input":"2024-09-15T10:17:18.739244Z","iopub.status.idle":"2024-09-15T10:17:26.482271Z","shell.execute_reply.started":"2024-09-15T10:17:18.739187Z","shell.execute_reply":"2024-09-15T10:17:26.480891Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n24/09/15 10:17:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","output_type":"stream"},{"name":"stdout","text":"<SparkContext master=local[1] appName=pyspark tutorial>\nSpark App Name : pyspark tutorial\n","output_type":"stream"}]},{"cell_type":"code","source":"data = [(\"Alice\", 25, \"New York\"),\n        (\"Raghu\", 30, \"Bengaluru\"),\n        (\"Ali\", 29, \"Pune\")]\n\n# Define the schema for the DataFrame\ncolumns = [\"Name\", \"Age\", \"City\"]\n\n# Create the DataFrame\ndf = spark.createDataFrame(data, columns)\n\n# Show the DataFrame\ndf.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-15T10:17:26.484746Z","iopub.execute_input":"2024-09-15T10:17:26.48516Z","iopub.status.idle":"2024-09-15T10:17:36.59145Z","shell.execute_reply.started":"2024-09-15T10:17:26.485114Z","shell.execute_reply":"2024-09-15T10:17:36.589185Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"+-----+---+---------+\n| Name|Age|     City|\n+-----+---+---------+\n|Alice| 25| New York|\n|Raghu| 30|Bengaluru|\n|  Ali| 29|     Pune|\n+-----+---+---------+\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 2. DataFrame Operations","metadata":{}},{"cell_type":"code","source":"df_csv = spark.read.csv(\"/kaggle/input/electric-power-consumption/powerconsumption.csv\", header=True, inferSchema=True)\n\n# Show the DataFrame\ndf_csv.show(5)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T10:17:36.593308Z","iopub.execute_input":"2024-09-15T10:17:36.593809Z","iopub.status.idle":"2024-09-15T10:17:40.715586Z","shell.execute_reply.started":"2024-09-15T10:17:36.593764Z","shell.execute_reply":"2024-09-15T10:17:40.713683Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"+-------------+-----------+--------+---------+-------------------+------------+----------------------+----------------------+----------------------+\n|     Datetime|Temperature|Humidity|WindSpeed|GeneralDiffuseFlows|DiffuseFlows|PowerConsumption_Zone1|PowerConsumption_Zone2|PowerConsumption_Zone3|\n+-------------+-----------+--------+---------+-------------------+------------+----------------------+----------------------+----------------------+\n|1/1/2017 0:00|      6.559|    73.8|    0.083|              0.051|       0.119|            34055.6962|           16128.87538|           20240.96386|\n|1/1/2017 0:10|      6.414|    74.5|    0.083|               0.07|       0.085|           29814.68354|           19375.07599|           20131.08434|\n|1/1/2017 0:20|      6.313|    74.5|     0.08|              0.062|         0.1|           29128.10127|           19006.68693|           19668.43373|\n|1/1/2017 0:30|      6.121|    75.0|    0.083|              0.091|       0.096|           28228.86076|           18361.09422|           18899.27711|\n|1/1/2017 0:40|      5.921|    75.7|    0.081|              0.048|       0.085|            27335.6962|           17872.34043|           18442.40964|\n+-------------+-----------+--------+---------+-------------------+------------+----------------------+----------------------+----------------------+\nonly showing top 5 rows\n\n","output_type":"stream"}]},{"cell_type":"code","source":"#filtering\n\ndf_wind = df_csv.filter(df_csv.WindSpeed > 2.0)\ndf_wind.show(10)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T10:17:40.721259Z","iopub.execute_input":"2024-09-15T10:17:40.722801Z","iopub.status.idle":"2024-09-15T10:17:41.475236Z","shell.execute_reply.started":"2024-09-15T10:17:40.722733Z","shell.execute_reply":"2024-09-15T10:17:41.473879Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"+--------------+-----------+--------+---------+-------------------+------------+----------------------+----------------------+----------------------+\n|      Datetime|Temperature|Humidity|WindSpeed|GeneralDiffuseFlows|DiffuseFlows|PowerConsumption_Zone1|PowerConsumption_Zone2|PowerConsumption_Zone3|\n+--------------+-----------+--------+---------+-------------------+------------+----------------------+----------------------+----------------------+\n|1/1/2017 10:10|      5.836|    71.3|     2.66|              257.9|       31.01|               25920.0|           15837.08207|           14428.91566|\n|1/1/2017 10:20|      5.996|   69.85|     4.93|              282.7|       31.96|           26393.92405|           16059.57447|           14671.80723|\n|1/1/2017 10:30|       6.22|   68.81|    4.924|              307.0|       32.42|           26861.77215|           16322.18845|           15036.14458|\n|1/1/2017 10:40|      6.703|   68.01|    4.923|              327.6|       33.22|           27511.89873|           16774.46809|           15267.46988|\n|1/1/2017 10:50|      6.993|   66.14|    4.918|              349.6|       33.41|           28149.87342|           17164.74164|           15244.33735|\n|1/1/2017 11:00|       7.54|   64.21|    4.916|              371.1|       33.43|           28714.93671|           17507.59878|            15591.3253|\n|1/1/2017 11:10|       8.22|    61.9|    4.916|              388.2|       33.89|           29043.03797|           17478.41945|           15816.86747|\n|1/1/2017 11:20|       9.49|    59.3|    2.451|              401.3|        34.4|           29261.77215|           17792.09726|           15932.53012|\n|1/6/2017 13:10|      15.22|   65.98|    2.287|              348.7|       308.3|           32409.11392|           21680.24316|           16811.56627|\n|1/6/2017 13:20|      15.38|   66.11|    4.708|              244.2|       234.6|           33022.78481|           21902.73556|           16724.81928|\n+--------------+-----------+--------+---------+-------------------+------------+----------------------+----------------------+----------------------+\nonly showing top 10 rows\n\n","output_type":"stream"}]},{"cell_type":"code","source":"#multiple filters\n\ndf_wind_temp = df_csv.filter((df_csv.WindSpeed > 2.0) & ((df_csv.Temperature >10)))\ndf_wind_temp.show(10)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T10:17:41.47707Z","iopub.execute_input":"2024-09-15T10:17:41.477616Z","iopub.status.idle":"2024-09-15T10:17:42.041399Z","shell.execute_reply.started":"2024-09-15T10:17:41.477555Z","shell.execute_reply":"2024-09-15T10:17:42.039848Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"+--------------+-----------+--------+---------+-------------------+------------+----------------------+----------------------+----------------------+\n|      Datetime|Temperature|Humidity|WindSpeed|GeneralDiffuseFlows|DiffuseFlows|PowerConsumption_Zone1|PowerConsumption_Zone2|PowerConsumption_Zone3|\n+--------------+-----------+--------+---------+-------------------+------------+----------------------+----------------------+----------------------+\n|1/6/2017 13:10|      15.22|   65.98|    2.287|              348.7|       308.3|           32409.11392|           21680.24316|           16811.56627|\n|1/6/2017 13:20|      15.38|   66.11|    4.708|              244.2|       234.6|           33022.78481|           21902.73556|           16724.81928|\n|1/20/2017 6:00|      10.09|   62.24|    2.981|              0.095|       0.082|           24157.97468|           15709.42249|           14839.51807|\n|1/20/2017 6:10|      10.73|   56.13|    4.917|              0.066|       0.115|           24394.93671|           15716.71733|            14631.3253|\n|1/20/2017 6:20|      11.12|   54.59|    4.918|              0.073|       0.093|           24218.73418|           15767.78116|           14446.26506|\n|1/20/2017 6:30|      11.18|   53.72|    4.917|              0.055|       0.078|           24534.68354|           15826.13982|           14336.38554|\n|1/20/2017 6:40|      11.42|   53.26|    4.916|              0.077|       0.134|           24802.02532|           16026.74772|           14284.33735|\n|1/20/2017 6:50|       11.4|   53.36|    4.914|              0.062|       0.152|            24467.8481|           15855.31915|           14174.45783|\n|1/20/2017 7:00|      11.52|   52.92|    4.923|              0.044|       0.093|           23945.31646|           15738.60182|           13786.98795|\n|1/20/2017 7:10|      11.58|   52.75|    4.918|              0.059|       0.093|            24024.3038|           16205.47112|           13197.10843|\n+--------------+-----------+--------+---------+-------------------+------------+----------------------+----------------------+----------------------+\nonly showing top 10 rows\n\n","output_type":"stream"}]},{"cell_type":"code","source":"df_air = spark.read.csv(\"/kaggle/input/air-quality/Air_Quality.csv\", header = True, inferSchema = True)\n\ndf_air.show(10)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T10:17:42.044312Z","iopub.execute_input":"2024-09-15T10:17:42.046926Z","iopub.status.idle":"2024-09-15T10:17:43.292265Z","shell.execute_reply.started":"2024-09-15T10:17:42.046864Z","shell.execute_reply":"2024-09-15T10:17:43.291088Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"+---------+------------+--------------------+--------------+------------+-------------+-----------+--------------------+-------------------+----------+----------+-------+\n|Unique ID|Indicator ID|                Name|       Measure|Measure Info|Geo Type Name|Geo Join ID|      Geo Place Name|        Time Period|Start_Date|Data Value|Message|\n+---------+------------+--------------------+--------------+------------+-------------+-----------+--------------------+-------------------+----------+----------+-------+\n|   179772|         640|Boiler Emissions-...|Number per km2|      number|        UHF42|        409|    Southeast Queens|               2015|01/01/2015|       0.3|   NULL|\n|   179785|         640|Boiler Emissions-...|Number per km2|      number|        UHF42|        209|Bensonhurst - Bay...|               2015|01/01/2015|       1.2|   NULL|\n|   178540|         365|Fine particles (P...|          Mean|      mcg/m3|        UHF42|        209|Bensonhurst - Bay...|Annual Average 2012|12/01/2011|       8.6|   NULL|\n|   178561|         365|Fine particles (P...|          Mean|      mcg/m3|        UHF42|        409|    Southeast Queens|Annual Average 2012|12/01/2011|       8.0|   NULL|\n|   823217|         365|Fine particles (P...|          Mean|      mcg/m3|        UHF42|        409|    Southeast Queens|        Summer 2022|06/01/2022|       6.1|   NULL|\n|   177910|         365|Fine particles (P...|          Mean|      mcg/m3|        UHF42|        209|Bensonhurst - Bay...|        Summer 2012|06/01/2012|      10.0|   NULL|\n|   177952|         365|Fine particles (P...|          Mean|      mcg/m3|        UHF42|        209|Bensonhurst - Bay...|        Summer 2013|06/01/2013|       9.8|   NULL|\n|   177973|         365|Fine particles (P...|          Mean|      mcg/m3|        UHF42|        409|    Southeast Queens|        Summer 2013|06/01/2013|       9.8|   NULL|\n|   177931|         365|Fine particles (P...|          Mean|      mcg/m3|        UHF42|        409|    Southeast Queens|        Summer 2012|06/01/2012|       9.6|   NULL|\n|   742274|         365|Fine particles (P...|          Mean|      mcg/m3|        UHF42|        410|           Rockaways|        Summer 2021|06/01/2021|       7.2|   NULL|\n+---------+------------+--------------------+--------------+------------+-------------+-----------+--------------------+-------------------+----------+----------+-------+\nonly showing top 10 rows\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# group by\n\nfrom pyspark.sql import functions as F\n\ngrouped_df = df_air.groupBy(\"Geo Place Name\").agg(\n    F.avg(\"Data Value\").alias(\"Average_Data_value\")\n)\n\n# Show the result\ngrouped_df.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-15T10:17:43.302877Z","iopub.execute_input":"2024-09-15T10:17:43.306145Z","iopub.status.idle":"2024-09-15T10:17:45.834133Z","shell.execute_reply.started":"2024-09-15T10:17:43.306084Z","shell.execute_reply":"2024-09-15T10:17:45.832917Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"[Stage 9:>                                                          (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"+--------------------+------------------+\n|      Geo Place Name|Average_Data_value|\n+--------------------+------------------+\n|     Northeast Bronx| 22.30780669144981|\n|Belmont and East ...|20.756363636363634|\n|Upper East Side (...| 25.25181818181818|\n|Greenwich Village...| 21.53529411764706|\n|              Queens|  19.3964705882353|\n|  East Harlem (CD11)| 23.88090909090909|\n|     Lower Manhattan|23.359999999999996|\n|         Northern SI|15.122222222222225|\n|Central Harlem - ...| 33.37063197026022|\n|Throgs Neck and C...|18.146363636363635|\n|South Crown Heigh...|19.159999999999997|\n|Stapleton - St. G...|20.343529411764713|\n|Bayside Little Ne...|15.822222222222228|\n|Fort Greene and B...|22.967272727272725|\n|       Midtown (CD5)|26.824545454545458|\n|Riverdale and Fie...|16.596363636363638|\n|Union Square - Lo...|29.379999999999985|\n|Williamsbridge an...| 17.22090909090909|\n|Highbridge and Co...|21.458181818181828|\n|  Washington Heights|24.133828996282514|\n+--------------------+------------------+\nonly showing top 20 rows\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"markdown","source":"# 3. Data Manipulation","metadata":{}},{"cell_type":"code","source":"#rename a column\n\ndf_air.withColumnRenamed('Geo Place Name', 'Place').show(5) #it returns a new dataframe","metadata":{"execution":{"iopub.status.busy":"2024-09-15T10:17:45.835482Z","iopub.execute_input":"2024-09-15T10:17:45.835895Z","iopub.status.idle":"2024-09-15T10:17:46.06839Z","shell.execute_reply.started":"2024-09-15T10:17:45.835849Z","shell.execute_reply":"2024-09-15T10:17:46.067216Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"+---------+------------+--------------------+--------------+------------+-------------+-----------+--------------------+-------------------+----------+----------+-------+\n|Unique ID|Indicator ID|                Name|       Measure|Measure Info|Geo Type Name|Geo Join ID|               Place|        Time Period|Start_Date|Data Value|Message|\n+---------+------------+--------------------+--------------+------------+-------------+-----------+--------------------+-------------------+----------+----------+-------+\n|   179772|         640|Boiler Emissions-...|Number per km2|      number|        UHF42|        409|    Southeast Queens|               2015|01/01/2015|       0.3|   NULL|\n|   179785|         640|Boiler Emissions-...|Number per km2|      number|        UHF42|        209|Bensonhurst - Bay...|               2015|01/01/2015|       1.2|   NULL|\n|   178540|         365|Fine particles (P...|          Mean|      mcg/m3|        UHF42|        209|Bensonhurst - Bay...|Annual Average 2012|12/01/2011|       8.6|   NULL|\n|   178561|         365|Fine particles (P...|          Mean|      mcg/m3|        UHF42|        409|    Southeast Queens|Annual Average 2012|12/01/2011|       8.0|   NULL|\n|   823217|         365|Fine particles (P...|          Mean|      mcg/m3|        UHF42|        409|    Southeast Queens|        Summer 2022|06/01/2022|       6.1|   NULL|\n+---------+------------+--------------------+--------------+------------+-------------+-----------+--------------------+-------------------+----------+----------+-------+\nonly showing top 5 rows\n\n","output_type":"stream"}]},{"cell_type":"code","source":"#select specific columns\n\ndf_air.select(\"Name\", \"Data Value\").show(5)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T10:17:46.069691Z","iopub.execute_input":"2024-09-15T10:17:46.070106Z","iopub.status.idle":"2024-09-15T10:17:46.281556Z","shell.execute_reply.started":"2024-09-15T10:17:46.070058Z","shell.execute_reply":"2024-09-15T10:17:46.280088Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"+--------------------+----------+\n|                Name|Data Value|\n+--------------------+----------+\n|Boiler Emissions-...|       0.3|\n|Boiler Emissions-...|       1.2|\n|Fine particles (P...|       8.6|\n|Fine particles (P...|       8.0|\n|Fine particles (P...|       6.1|\n+--------------------+----------+\nonly showing top 5 rows\n\n","output_type":"stream"}]},{"cell_type":"code","source":"#drop a column\n\ndf_air.drop('Message').show(5)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T10:17:46.282994Z","iopub.execute_input":"2024-09-15T10:17:46.28346Z","iopub.status.idle":"2024-09-15T10:17:46.587271Z","shell.execute_reply.started":"2024-09-15T10:17:46.283405Z","shell.execute_reply":"2024-09-15T10:17:46.586129Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"+---------+------------+--------------------+--------------+------------+-------------+-----------+--------------------+-------------------+----------+----------+\n|Unique ID|Indicator ID|                Name|       Measure|Measure Info|Geo Type Name|Geo Join ID|      Geo Place Name|        Time Period|Start_Date|Data Value|\n+---------+------------+--------------------+--------------+------------+-------------+-----------+--------------------+-------------------+----------+----------+\n|   179772|         640|Boiler Emissions-...|Number per km2|      number|        UHF42|        409|    Southeast Queens|               2015|01/01/2015|       0.3|\n|   179785|         640|Boiler Emissions-...|Number per km2|      number|        UHF42|        209|Bensonhurst - Bay...|               2015|01/01/2015|       1.2|\n|   178540|         365|Fine particles (P...|          Mean|      mcg/m3|        UHF42|        209|Bensonhurst - Bay...|Annual Average 2012|12/01/2011|       8.6|\n|   178561|         365|Fine particles (P...|          Mean|      mcg/m3|        UHF42|        409|    Southeast Queens|Annual Average 2012|12/01/2011|       8.0|\n|   823217|         365|Fine particles (P...|          Mean|      mcg/m3|        UHF42|        409|    Southeast Queens|        Summer 2022|06/01/2022|       6.1|\n+---------+------------+--------------------+--------------+------------+-------------+-----------+--------------------+-------------------+----------+----------+\nonly showing top 5 rows\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Adding Columns\n\n1. A constant value\n2. Based on a condition","metadata":{}},{"cell_type":"code","source":"data = [(\"Alice\", 25, \"New York\", 25000),\n        (\"Raghu\", 30, \"Bengaluru\", 45000),\n        (\"Ali\", 29, \"Pune\", 36000),\n       (\"Dominic\", 20, \"London\", 25000),\n       (\"Sara\", 19, \"Tunis\", 12000)]\n\ncolumns = [\"Name\", \"Age\", \"City\", \"Salary\"]\n\ndf = spark.createDataFrame(data, columns)\n\ndf.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-15T10:17:46.588615Z","iopub.execute_input":"2024-09-15T10:17:46.589058Z","iopub.status.idle":"2024-09-15T10:17:47.079215Z","shell.execute_reply.started":"2024-09-15T10:17:46.589007Z","shell.execute_reply":"2024-09-15T10:17:47.078012Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"+-------+---+---------+------+\n|   Name|Age|     City|Salary|\n+-------+---+---------+------+\n|  Alice| 25| New York| 25000|\n|  Raghu| 30|Bengaluru| 45000|\n|    Ali| 29|     Pune| 36000|\n|Dominic| 20|   London| 25000|\n|   Sara| 19|    Tunis| 12000|\n+-------+---+---------+------+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"#Add column with constant value\nfrom pyspark.sql.functions import lit, when, mean\n\ndf.withColumn('Favorite Music Genre', lit(\"Pop\")).show(5)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T10:17:47.080784Z","iopub.execute_input":"2024-09-15T10:17:47.083439Z","iopub.status.idle":"2024-09-15T10:17:47.467399Z","shell.execute_reply.started":"2024-09-15T10:17:47.083384Z","shell.execute_reply":"2024-09-15T10:17:47.466271Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"+-------+---+---------+------+--------------------+\n|   Name|Age|     City|Salary|Favorite Music Genre|\n+-------+---+---------+------+--------------------+\n|  Alice| 25| New York| 25000|                 Pop|\n|  Raghu| 30|Bengaluru| 45000|                 Pop|\n|    Ali| 29|     Pune| 36000|                 Pop|\n|Dominic| 20|   London| 25000|                 Pop|\n|   Sara| 19|    Tunis| 12000|                 Pop|\n+-------+---+---------+------+--------------------+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Add column based on condition\n\ndf.withColumn('Generation', when((df.Age > 12) & (df.Age < 27), \"Gen Z\")\n             .when((df.Age > 28) & (df.Age < 43), \"Millennials\")).show(5)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T10:17:47.469373Z","iopub.execute_input":"2024-09-15T10:17:47.470667Z","iopub.status.idle":"2024-09-15T10:17:47.930145Z","shell.execute_reply.started":"2024-09-15T10:17:47.470609Z","shell.execute_reply":"2024-09-15T10:17:47.928952Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"+-------+---+---------+------+-----------+\n|   Name|Age|     City|Salary| Generation|\n+-------+---+---------+------+-----------+\n|  Alice| 25| New York| 25000|      Gen Z|\n|  Raghu| 30|Bengaluru| 45000|Millennials|\n|    Ali| 29|     Pune| 36000|Millennials|\n|Dominic| 20|   London| 25000|      Gen Z|\n|   Sara| 19|    Tunis| 12000|      Gen Z|\n+-------+---+---------+------+-----------+\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 4. Handling Missing Data","metadata":{}},{"cell_type":"code","source":"data = [(\"Alice\", 25, \"New York\", 25000),\n        (\"Raghu\", None, \"Bengaluru\", 45000),\n        (\"Ali\", 29, None, 36000),\n        (None, 20, \"London\", 25000),\n        (\"Sara\", 19, \"Tunis\", None)]\n\ncolumns = [\"Name\", \"Age\", \"City\", \"Salary\"]\n\ndf = spark.createDataFrame(data, columns)\n\ndf.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-15T10:17:47.931513Z","iopub.execute_input":"2024-09-15T10:17:47.931962Z","iopub.status.idle":"2024-09-15T10:17:48.334605Z","shell.execute_reply.started":"2024-09-15T10:17:47.931911Z","shell.execute_reply":"2024-09-15T10:17:48.333451Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"+-----+----+---------+------+\n| Name| Age|     City|Salary|\n+-----+----+---------+------+\n|Alice|  25| New York| 25000|\n|Raghu|NULL|Bengaluru| 45000|\n|  Ali|  29|     NULL| 36000|\n| NULL|  20|   London| 25000|\n| Sara|  19|    Tunis|  NULL|\n+-----+----+---------+------+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# dropping rows with missing values\n\ndf_dropped = df.dropna()\ndf_dropped.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-15T10:17:48.335865Z","iopub.execute_input":"2024-09-15T10:17:48.336295Z","iopub.status.idle":"2024-09-15T10:17:48.740127Z","shell.execute_reply.started":"2024-09-15T10:17:48.336247Z","shell.execute_reply":"2024-09-15T10:17:48.738864Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"+-----+---+--------+------+\n| Name|Age|    City|Salary|\n+-----+---+--------+------+\n|Alice| 25|New York| 25000|\n+-----+---+--------+------+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# filling missing values with specific values\n\nmean_salary = df.select(mean(df['Salary'])).collect()[0][0]\n\ndf_filled = df.fillna({\n    \"Name\": \"Unknown\",  # Fill missing names with \"Unknown\"\n    \"Age\": 0,           # Fill missing ages with 0\n    \"City\": \"Unknown\",   # Fill missing cities with \"Unknown\"\n    \"Salary\": mean_salary         # Fill missing salaries with mean salary\n})\n\ndf_filled.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-15T10:17:48.741407Z","iopub.execute_input":"2024-09-15T10:17:48.74344Z","iopub.status.idle":"2024-09-15T10:17:49.732014Z","shell.execute_reply.started":"2024-09-15T10:17:48.743385Z","shell.execute_reply":"2024-09-15T10:17:49.730462Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"+-------+---+---------+------+\n|   Name|Age|     City|Salary|\n+-------+---+---------+------+\n|  Alice| 25| New York| 25000|\n|  Raghu|  0|Bengaluru| 45000|\n|    Ali| 29|  Unknown| 36000|\n|Unknown| 20|   London| 25000|\n|   Sara| 19|    Tunis| 32750|\n+-------+---+---------+------+\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 5. Joins and Aggregations","metadata":{}},{"cell_type":"code","source":"data1 = [(1, 'Chocolate', 200.00, 'Food'),\n        (2, 'Cookies', 100.00, 'Food'),\n        (3, 'Journal', 250.00, 'Stationary'),\n        (4, 'Energy Drink', 300.00, 'Beverage'),\n        (5, 'Noodles', 230.00, 'Food')]\n\ndata1Cols = ['item_id', 'item_name', 'item_price', 'item_category']\n\nstore1 = spark.createDataFrame(data1, data1Cols)\n\nstore1.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-15T10:17:49.73377Z","iopub.execute_input":"2024-09-15T10:17:49.734286Z","iopub.status.idle":"2024-09-15T10:17:50.123658Z","shell.execute_reply.started":"2024-09-15T10:17:49.734175Z","shell.execute_reply":"2024-09-15T10:17:50.122461Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"+-------+------------+----------+-------------+\n|item_id|   item_name|item_price|item_category|\n+-------+------------+----------+-------------+\n|      1|   Chocolate|     200.0|         Food|\n|      2|     Cookies|     100.0|         Food|\n|      3|     Journal|     250.0|   Stationary|\n|      4|Energy Drink|     300.0|     Beverage|\n|      5|     Noodles|     230.0|         Food|\n+-------+------------+----------+-------------+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"data2 = [(1, 5),\n        (2, 3),\n        (3, 2),\n        (4, 10),\n        (5, 7),\n        (6,5),\n        (7,10)]\n\ndata2Cols = ['item_id', 'item_quantity']\n\nstore2 = spark.createDataFrame(data2, data2Cols)\n\nstore2.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-15T10:17:50.125219Z","iopub.execute_input":"2024-09-15T10:17:50.127247Z","iopub.status.idle":"2024-09-15T10:17:50.573093Z","shell.execute_reply.started":"2024-09-15T10:17:50.12719Z","shell.execute_reply":"2024-09-15T10:17:50.571794Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"+-------+-------------+\n|item_id|item_quantity|\n+-------+-------------+\n|      1|            5|\n|      2|            3|\n|      3|            2|\n|      4|           10|\n|      5|            7|\n|      6|            5|\n|      7|           10|\n+-------+-------------+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# inner join\n\nstore1.join(store2, store1.item_id == store2.item_id, \"inner\").show()","metadata":{"execution":{"iopub.status.busy":"2024-09-15T10:17:50.574473Z","iopub.execute_input":"2024-09-15T10:17:50.575093Z","iopub.status.idle":"2024-09-15T10:17:51.664965Z","shell.execute_reply.started":"2024-09-15T10:17:50.575037Z","shell.execute_reply":"2024-09-15T10:17:51.663767Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"+-------+------------+----------+-------------+-------+-------------+\n|item_id|   item_name|item_price|item_category|item_id|item_quantity|\n+-------+------------+----------+-------------+-------+-------------+\n|      1|   Chocolate|     200.0|         Food|      1|            5|\n|      2|     Cookies|     100.0|         Food|      2|            3|\n|      3|     Journal|     250.0|   Stationary|      3|            2|\n|      4|Energy Drink|     300.0|     Beverage|      4|           10|\n|      5|     Noodles|     230.0|         Food|      5|            7|\n+-------+------------+----------+-------------+-------+-------------+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# left join\n\nstore1.join(store2, store1.item_id == store2.item_id, \"left\").show()","metadata":{"execution":{"iopub.status.busy":"2024-09-15T10:17:51.666459Z","iopub.execute_input":"2024-09-15T10:17:51.666918Z","iopub.status.idle":"2024-09-15T10:17:52.606804Z","shell.execute_reply.started":"2024-09-15T10:17:51.666864Z","shell.execute_reply":"2024-09-15T10:17:52.605465Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"+-------+------------+----------+-------------+-------+-------------+\n|item_id|   item_name|item_price|item_category|item_id|item_quantity|\n+-------+------------+----------+-------------+-------+-------------+\n|      5|     Noodles|     230.0|         Food|      5|            7|\n|      1|   Chocolate|     200.0|         Food|      1|            5|\n|      3|     Journal|     250.0|   Stationary|      3|            2|\n|      2|     Cookies|     100.0|         Food|      2|            3|\n|      4|Energy Drink|     300.0|     Beverage|      4|           10|\n+-------+------------+----------+-------------+-------+-------------+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# right join\n\nstore1.join(store2, store1.item_id == store2.item_id, \"right\").show()","metadata":{"execution":{"iopub.status.busy":"2024-09-15T10:17:52.608075Z","iopub.execute_input":"2024-09-15T10:17:52.608553Z","iopub.status.idle":"2024-09-15T10:17:53.794255Z","shell.execute_reply.started":"2024-09-15T10:17:52.608501Z","shell.execute_reply":"2024-09-15T10:17:53.793103Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"+-------+------------+----------+-------------+-------+-------------+\n|item_id|   item_name|item_price|item_category|item_id|item_quantity|\n+-------+------------+----------+-------------+-------+-------------+\n|   NULL|        NULL|      NULL|         NULL|      7|           10|\n|   NULL|        NULL|      NULL|         NULL|      6|            5|\n|      5|     Noodles|     230.0|         Food|      5|            7|\n|      1|   Chocolate|     200.0|         Food|      1|            5|\n|      3|     Journal|     250.0|   Stationary|      3|            2|\n|      2|     Cookies|     100.0|         Food|      2|            3|\n|      4|Energy Drink|     300.0|     Beverage|      4|           10|\n+-------+------------+----------+-------------+-------+-------------+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"join_df = store1.join(store2, on='item_id')\njoin_df.withColumn('total_revenue', join_df.item_price * join_df.item_quantity).show()","metadata":{"execution":{"iopub.status.busy":"2024-09-15T10:17:53.795649Z","iopub.execute_input":"2024-09-15T10:17:53.796077Z","iopub.status.idle":"2024-09-15T10:17:55.188548Z","shell.execute_reply.started":"2024-09-15T10:17:53.796029Z","shell.execute_reply":"2024-09-15T10:17:55.187382Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"[Stage 44:>                                                         (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"+-------+------------+----------+-------------+-------------+-------------+\n|item_id|   item_name|item_price|item_category|item_quantity|total_revenue|\n+-------+------------+----------+-------------+-------------+-------------+\n|      1|   Chocolate|     200.0|         Food|            5|       1000.0|\n|      2|     Cookies|     100.0|         Food|            3|        300.0|\n|      3|     Journal|     250.0|   Stationary|            2|        500.0|\n|      4|Energy Drink|     300.0|     Beverage|           10|       3000.0|\n|      5|     Noodles|     230.0|         Food|            7|       1610.0|\n+-------+------------+----------+-------------+-------------+-------------+\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"markdown","source":"# 6. Working with RDDs (Basic)","metadata":{}},{"cell_type":"markdown","source":"**Will load a text file and count the number of occurences of a word\nin this text file**","metadata":{}},{"cell_type":"code","source":"textFile = '''\nThis is a sample text file to learn basic map and\nreduce function, map and reduce will be used\nto calculate the number of occurences of a word\nin this text file\n'''\n\nwith open('mpText.txt', 'w') as f:\n    f.write(textFile)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T10:18:52.294257Z","iopub.execute_input":"2024-09-15T10:18:52.295307Z","iopub.status.idle":"2024-09-15T10:18:52.301766Z","shell.execute_reply.started":"2024-09-15T10:18:52.295251Z","shell.execute_reply":"2024-09-15T10:18:52.300616Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"rdd = spark.sparkContext.textFile('/kaggle/working/mpText.txt')","metadata":{"execution":{"iopub.status.busy":"2024-09-15T10:18:52.542505Z","iopub.execute_input":"2024-09-15T10:18:52.543398Z","iopub.status.idle":"2024-09-15T10:18:52.607779Z","shell.execute_reply.started":"2024-09-15T10:18:52.543325Z","shell.execute_reply":"2024-09-15T10:18:52.606417Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# Split each line into words\nwords_rdd = rdd.flatMap(lambda line: line.split())\n\n# Map each word to a tuple (word, 1) and reduce by key to count occurrences\nword_count_rdd = words_rdd.map(lambda word: (word.lower(), 1)).reduceByKey(lambda a, b: a + b)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T10:18:53.382679Z","iopub.execute_input":"2024-09-15T10:18:53.383118Z","iopub.status.idle":"2024-09-15T10:18:53.473355Z","shell.execute_reply.started":"2024-09-15T10:18:53.38308Z","shell.execute_reply":"2024-09-15T10:18:53.472089Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"word_count_rdd.saveAsTextFile(\"word_count_output\")","metadata":{"execution":{"iopub.status.busy":"2024-09-15T10:19:26.168718Z","iopub.execute_input":"2024-09-15T10:19:26.169163Z","iopub.status.idle":"2024-09-15T10:19:27.161835Z","shell.execute_reply.started":"2024-09-15T10:19:26.169124Z","shell.execute_reply":"2024-09-15T10:19:27.160568Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"for word, count in word_count_rdd.collect():\n    print(f\"{word}: {count}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-15T10:19:45.82214Z","iopub.execute_input":"2024-09-15T10:19:45.82316Z","iopub.status.idle":"2024-09-15T10:19:46.11488Z","shell.execute_reply.started":"2024-09-15T10:19:45.823108Z","shell.execute_reply":"2024-09-15T10:19:46.113507Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"this: 2\nis: 1\na: 2\nsample: 1\ntext: 2\nfile: 2\nto: 2\nlearn: 1\nbasic: 1\nmap: 2\nand: 2\nreduce: 2\nfunction,: 1\nwill: 1\nbe: 1\nused: 1\ncalculate: 1\nthe: 1\nnumber: 1\nof: 2\noccurences: 1\nword: 1\nin: 1\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}